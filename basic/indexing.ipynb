{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c911a9b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔍 Apa Itu Indexing dalam RAG\n",
    "\n",
    "**Indexing** adalah proses **mengorganisasi dan menyimpan informasi dari dokumen dalam bentuk yang mudah dicari kembali** ketika dibutuhkan oleh sistem RAG.\n",
    "Dalam konteks RAG, indexing bukan hanya menyimpan teks, tapi **menyimpan pemahaman makna dari teks** (dalam bentuk vektor numerik yang mewakili isi dokumen secara semantik).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Mengapa Indexing Diperlukan\n",
    "\n",
    "Model bahasa besar (LLM) seperti GPT tidak menyimpan atau mengingat semua informasi eksternal — kapasitas memorinya terbatas pada data pelatihan.\n",
    "Ketika kita ingin agar model bisa menjawab pertanyaan berdasarkan sumber eksternal (misalnya kumpulan artikel, jurnal, atau dokumen perusahaan), maka:\n",
    "\n",
    "* Kita **tidak bisa** setiap kali membaca semua dokumen dari awal.\n",
    "* Kita perlu **cara cepat untuk menemukan bagian yang relevan** saja.\n",
    "\n",
    "Di sinilah indexing berperan:\n",
    "Ia membuat sistem dapat **menemukan informasi yang relevan dalam hitungan milidetik** dari ribuan bahkan jutaan potongan data.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Bagaimana Sistematikanya dalam Aliran Data (Data Flow)\n",
    "\n",
    "Proses indexing dalam RAG mengikuti **alur sistematis dari data mentah menjadi data siap pakai untuk pencarian semantik**, seperti ini:\n",
    "\n",
    "1. ### 🧾 **Pengumpulan Data (Data Collection)**\n",
    "\n",
    "   Sistem mengumpulkan berbagai sumber teks, misalnya artikel blog, dokumen PDF, atau halaman web.\n",
    "   Ini adalah bahan mentah yang nanti akan diindeks.\n",
    "\n",
    "2. ### ✂️ **Pemecahan Dokumen (Chunking)**\n",
    "\n",
    "   Dokumen panjang dipecah menjadi bagian kecil agar setiap bagian bisa berdiri sendiri secara makna.\n",
    "   Ini penting karena sistem retrieval bekerja lebih akurat ketika setiap unit informasi cukup spesifik.\n",
    "\n",
    "3. ### 🧬 **Representasi Semantik (Embedding)**\n",
    "\n",
    "   Setiap potongan teks diubah menjadi representasi matematis (biasanya vektor berdimensi tinggi).\n",
    "   Vektor ini menyimpan *makna semantik*, bukan sekadar kata per kata.\n",
    "\n",
    "   Misalnya, kalimat “cara kerja otak buatan” dan “mekanisme AI berpikir” akan memiliki representasi yang mirip — karena maknanya serupa.\n",
    "\n",
    "4. ### 🗂️ **Penyimpanan dalam Basis Data Vektor (Vector Store)**\n",
    "\n",
    "   Semua vektor hasil embedding disimpan dalam struktur khusus bernama **vector database** (seperti FAISS, Pinecone, Chroma, dsb).\n",
    "   Database ini memungkinkan sistem mencari kesamaan makna antara teks pertanyaan dan potongan teks yang telah diindeks.\n",
    "\n",
    "5. ### ⚡ **Retrieval**\n",
    "\n",
    "   Ketika pengguna mengajukan pertanyaan, pertanyaan tersebut juga diubah menjadi vektor.\n",
    "   Sistem kemudian mencari potongan teks di dalam index yang paling mirip secara semantik dengan vektor pertanyaan tersebut.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Kegunaan Indexing\n",
    "\n",
    "| Tujuan                                     | Penjelasan                                                                                                                                                |\n",
    "| ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Efisiensi pencarian**                    | Tanpa indexing, sistem harus membaca seluruh teks setiap kali ada pertanyaan — ini sangat lambat. Indexing membuat pencarian bisa dilakukan dengan cepat. |\n",
    "| **Pencocokan makna, bukan kata**           | Karena indexing menggunakan embedding semantik, sistem dapat memahami kesamaan makna meskipun kata-katanya berbeda.                                       |\n",
    "| **Fondasi bagi Retrieval**                 | Index adalah dasar dari proses retrieval. Tanpa index, tidak ada cara efisien untuk menemukan informasi relevan dari data besar.                          |\n",
    "| **Menjembatani LLM dengan data eksternal** | Indexing memungkinkan RAG mengakses dan menggunakan data yang tidak termasuk dalam pelatihan model.                                                       |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Hubungan Indexing dengan Komponen RAG\n",
    "\n",
    "Sistem RAG terbagi menjadi dua komponen besar:\n",
    "\n",
    "1. **Retrieval (pengambilan informasi)**\n",
    "   Bergantung sepenuhnya pada hasil indexing. Tanpa index yang baik, retrieval akan lambat dan tidak akurat.\n",
    "\n",
    "2. **Generation (pembuatan jawaban)**\n",
    "   Menggunakan hasil retrieval sebagai konteks untuk menghasilkan jawaban yang informatif dan faktual.\n",
    "\n",
    "Dengan kata lain:\n",
    "\n",
    "> **Indexing adalah fondasi yang membuat retrieval mungkin, dan retrieval adalah fondasi yang membuat generasi jawaban relevan.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Analogi untuk Memudahkan\n",
    "\n",
    "Bayangkan kamu punya **perpustakaan besar** dengan ribuan buku.\n",
    "\n",
    "* Jika tidak ada katalog, kamu harus membuka satu per satu buku untuk mencari topik tertentu — lambat dan melelahkan.\n",
    "* Jika kamu membuat **katalog tematik**, kamu bisa langsung tahu di rak mana dan halaman mana informasi itu berada.\n",
    "\n",
    "Nah, **indexing adalah pembuatan katalog digital semantik** — bukan hanya berdasarkan kata, tapi berdasarkan *makna isi teks*.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Kesimpulan\n",
    "\n",
    "| Aspek                | Penjelasan Singkat                                                                                 |\n",
    "| -------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **Pengertian**       | Proses mengubah teks menjadi bentuk terstruktur dan bermakna agar mudah dicari kembali             |\n",
    "| **Kegunaan**         | Mempercepat pencarian, meningkatkan relevansi hasil, dan memungkinkan RAG mengakses data eksternal |\n",
    "| **Sistematika Data** | Data mentah → Chunk → Embedding → Vector Database → Query & Retrieval                              |\n",
    "| **Fungsi dalam RAG** | Menjadi dasar bagi proses retrieval, yang menyediakan konteks untuk generasi jawaban               |\n",
    "\n",
    "---\n",
    "## 🧠 1. Mengapa Kualitas Indexing Penting\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) itu pada dasarnya bekerja dalam dua tahap:\n",
    "\n",
    "1. **Retrieval** — menemukan informasi relevan.\n",
    "2. **Generation** — membuat jawaban berdasarkan hasil retrieval.\n",
    "\n",
    "Kalau tahap pertama (retrieval) salah atau kurang relevan,\n",
    "maka tahap kedua (generation) juga akan salah arah.\n",
    "\n",
    "> **Kualitas jawaban RAG sangat tergantung pada kualitas indexing.**\n",
    "> Sistem hanya bisa menghasilkan jawaban sebaik konteks yang diambilnya.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2. Faktor-Faktor yang Mempengaruhi Kualitas Indexing\n",
    "\n",
    "Mari kita uraikan satu per satu secara konseptual 👇\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 a. **Ukuran dan Tumpang Tindih (Chunk Size & Overlap)**\n",
    "\n",
    "Saat dokumen dipecah menjadi potongan kecil (chunk), dua hal perlu diperhatikan:\n",
    "\n",
    "| Aspek         | Terlalu Kecil            | Terlalu Besar                                 |\n",
    "| ------------- | ------------------------ | --------------------------------------------- |\n",
    "| **Kelebihan** | Relevansi lebih spesifik | Konteks utuh lebih terjaga                    |\n",
    "| **Kelemahan** | Hilang konteks makna     | Sulit ditemukan karena embedding terlalu umum |\n",
    "\n",
    "Idealnya:\n",
    "\n",
    "* **Chunk size**: 300–800 kata (tergantung jenis dokumen)\n",
    "* **Overlap**: 10–20% antar chunk agar makna tidak terpotong\n",
    "\n",
    "> 📘 Contoh:\n",
    "> Kalau kamu mengindeks artikel panjang di Substack, terlalu kecil chunk-nya bisa bikin makna paragraf terpisah dari konteks.\n",
    "> Tapi terlalu besar bisa bikin model kesulitan menentukan bagian mana yang relevan dengan query.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧬 b. **Kualitas Embedding**\n",
    "\n",
    "Embedding adalah representasi semantik dari teks dalam bentuk vektor numerik.\n",
    "\n",
    "Kualitas embedding menentukan seberapa “pintar” sistem memahami makna teks.\n",
    "\n",
    "| Jenis Embedding                                           | Karakteristik                                            |\n",
    "| --------------------------------------------------------- | -------------------------------------------------------- |\n",
    "| **Statistik sederhana (TF-IDF, BM25)**                    | Berdasarkan kata — cepat tapi kurang memahami makna      |\n",
    "| **Embedding neural (mis. OpenAI, Cohere, Sentence-BERT)** | Berdasarkan makna — lebih akurat dan relevan             |\n",
    "| **Domain-specific embedding**                             | Dilatih untuk bidang tertentu, misalnya medis atau hukum |\n",
    "\n",
    "> Semakin baik embedding, semakin akurat hasil pencarian semantik (semantic search).\n",
    "\n",
    "---\n",
    "\n",
    "### 🗃️ c. **Struktur dan Jenis Database**\n",
    "\n",
    "Indexing juga dipengaruhi oleh bagaimana dan di mana data disimpan.\n",
    "\n",
    "| Database                         | Ciri Khas                                    |\n",
    "| -------------------------------- | -------------------------------------------- |\n",
    "| **FAISS**                        | Cepat, cocok untuk eksperimen lokal          |\n",
    "| **Pinecone / Weaviate / Qdrant** | Cloud-based, scalable, mendukung metadata    |\n",
    "| **Chroma**                       | Ringan, cocok untuk project kecil atau lokal |\n",
    "\n",
    "Database ini tidak hanya menyimpan embedding, tapi juga **metadata** (judul, tanggal, sumber, dll) yang bisa membantu sistem menyaring hasil retrieval lebih tepat.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ d. **Strategi Preprocessing dan Normalisasi Teks**\n",
    "\n",
    "Sebelum di-*index*, teks biasanya perlu dibersihkan:\n",
    "\n",
    "* Menghapus HTML tag, script, dan simbol aneh\n",
    "* Menormalkan huruf besar/kecil\n",
    "* Menghapus redundansi (misal footer atau navigasi situs)\n",
    "\n",
    "Kalau tidak dibersihkan, hasil embedding bisa “berisik” dan menurunkan kualitas pencarian.\n",
    "\n",
    "> Contoh: kalau kamu scrape blog Medium dan tidak buang bagian seperti “login to continue” atau “related articles”, embedding-nya akan terdistorsi.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 e. **Kualitas dan Keberagaman Data yang Diindeks**\n",
    "\n",
    "Kalau sumber datanya sempit atau tidak representatif, sistem hanya akan bisa menjawab sebagian kecil pertanyaan dengan baik.\n",
    "\n",
    "Idealnya:\n",
    "\n",
    "* Data yang diindeks **beragam** (berisi berbagai topik dalam domain)\n",
    "* Tapi juga **konsisten secara kualitas** (tidak banyak teks spam, duplikat, atau noise)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 f. **Metode Pencarian (Retrieval Method)**\n",
    "\n",
    "Setelah indexing selesai, metode pencarian juga berpengaruh.\n",
    "\n",
    "Ada dua pendekatan umum:\n",
    "\n",
    "1. **Similarity-based retrieval** — cari chunk yang paling mirip dengan query.\n",
    "2. **Hybrid retrieval** — gabungkan pencarian semantik (embedding) dan pencarian berbasis kata (keyword).\n",
    "\n",
    "Hybrid biasanya memberikan hasil paling relevan karena menggabungkan “pemahaman makna” dan “kecocokan kata”.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 3. Dampak Langsung Kualitas Indexing terhadap RAG\n",
    "\n",
    "| Aspek                      | Indexing Buruk                       | Indexing Baik                                     |\n",
    "| -------------------------- | ------------------------------------ | ------------------------------------------------- |\n",
    "| **Kecepatan pencarian**    | Lambat karena data tidak terstruktur | Cepat, hanya mencari di bagian relevan            |\n",
    "| **Akurasi konteks**        | Jawaban salah atau tidak nyambung    | Jawaban relevan dan informatif                    |\n",
    "| **Relevansi semantik**     | Tidak memahami makna sebenarnya      | Mengerti sinonim dan makna tersirat               |\n",
    "| **Kemampuan generalisasi** | Terbatas, banyak “miss”              | Mampu menjawab dari berbagai formulasi pertanyaan |\n",
    "| **Pengalaman pengguna**    | Frustrasi, tidak dapat jawaban tepat | Lancar, terasa “cerdas” dan natural               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 4. Inti Hubungan: Indexing → Retrieval → Generation\n",
    "\n",
    "Kamu bisa bayangkan seperti rantai logika:\n",
    "\n",
    "```\n",
    "Indexing yang baik \n",
    "     ↓\n",
    "Retrieval yang akurat \n",
    "     ↓\n",
    "Konteks berkualitas tinggi \n",
    "     ↓\n",
    "Jawaban LLM yang relevan dan faktual\n",
    "```\n",
    "\n",
    "Jadi, RAG bukan hanya soal kemampuan LLM menjawab,\n",
    "tapi juga **seberapa baik sistem memahami dan mengatur pengetahuannya melalui indexing.**\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Kesimpulan Akhir\n",
    "\n",
    "| Aspek               | Penjelasan Ringkas                                                      |\n",
    "| ------------------- | ----------------------------------------------------------------------- |\n",
    "| **Tujuan utama**    | Membuat informasi dapat dicari dan dipahami secara semantik             |\n",
    "| **Mengapa penting** | Tanpa indexing, RAG tidak bisa mengambil konteks relevan dengan efisien |\n",
    "| **Faktor utama**    | Chunk size, embedding, database, preprocessing, metode retrieval        |\n",
    "| **Dampaknya**       | Langsung menentukan kualitas, relevansi, dan kecepatan jawaban model    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903c889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Artikel 1:\n",
      "https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/\n",
      "Content length: 32931\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Artikel 2:\n",
      "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\n",
      "Content length: 28922\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Artikel 3:\n",
      "https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\n",
      "Content length: 50155\n",
      "--------------------------------------------------------------------------------\n",
      "📘 Artikel 4:\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Content length: 43047\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "    \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"📘 Artikel {i}:\")\n",
    "    print(doc.metadata[\"source\"])\n",
    "    print(f\"Content length: {len(doc.page_content)}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad97c10",
   "metadata": {},
   "source": [
    "## Prepare requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1e126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5eab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "🧹 Cleaning with FLAN-T5:   0%|          | 0/4 [00:00<?, ?it/s]Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "🧹 Cleaning with FLAN-T5:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "🧹 Cleaning with FLAN-T5:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "🧹 Cleaning with FLAN-T5:  75%|███████▌  | 3/4 [00:39<00:17, 17.08s/it]Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "🧹 Cleaning with FLAN-T5: 100%|██████████| 4/4 [01:14<00:00, 18.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm_cleaner = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "def text_cleaner(docs, model):\n",
    "    cleaned_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"🧹 Cleaning with FLAN-T5\"):\n",
    "        raw = doc.page_content.strip()\n",
    "\n",
    "        prompt = (\n",
    "            \"Clean and normalize the following text. \"\n",
    "            \"Remove noise, HTML artifacts, redundant whitespace, \"\n",
    "            \"and improve readability:\\n\\n\" + raw[:4000]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            output = model(prompt, max_length=1024, truncation=True)[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            output = f\"[ERROR cleaning text: {e}]\"\n",
    "\n",
    "        # kembalikan ke format Document\n",
    "        cleaned_docs.append(\n",
    "            Document(page_content=output, metadata=getattr(doc, \"metadata\", {}))\n",
    "        )\n",
    "\n",
    "    return cleaned_docs\n",
    "docs=text_cleaner(docs, llm_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1665ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5311/2134279852.py:8: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# prepare embeding\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "chunking_model=\"BAAI/bge-m3\"\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=chunking_model,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f7606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and index document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "splits=splitter.split_documents(docs)\n",
    "\n",
    "vectorstore=Chroma.from_documents(splits, embeddings)\n",
    "\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60007208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Document 1\n",
      "Source: https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\n",
      "Content:\n",
      "Exploration Strategies in Deep Reinforcement Learning Date: June 7, 2020 | Estimated Reading Time: 36 min | Author: Lilian Weng [Updated on 2020-06-17: Add “exploration via disagreement” in the “Forward Dynamics” section. Exploration versus exploration is a critical topic in Reinforcement Learning. We’d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to lo...\n",
      "--------------------------------------------------------------------------------\n",
      "📄 Document 2\n",
      "Source: https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\n",
      "Content:\n",
      "ox was fat....\n",
      "--------------------------------------------------------------------------------\n",
      "📄 Document 3\n",
      "Source: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "Content:\n",
      "LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In a LLM-powered auto...\n",
      "--------------------------------------------------------------------------------\n",
      "📄 Document 4\n",
      "Source: https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/\n",
      "Content:\n",
      "Toxic content should not be classified as harmful....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"RAG?\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"📄 Document {i}\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Content:\\n{doc.page_content[:500]}...\")  # tampilkan sebagian isi\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142818db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ca3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(api_key=groq_api_key, model='moonshotai/kimi-k2-instruct-0905', temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01824891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Berdasarkan konteks yang diberikan, tidak ada informasi yang menjelaskan \"apa itu RAG\".', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 481, 'total_tokens': 507, 'completion_time': 0.072002125, 'prompt_time': 0.026949604, 'queue_time': 0.246820205, 'total_time': 0.098951729, 'prompt_tokens_details': {'cached_tokens': 256}}, 'model_name': 'moonshotai/kimi-k2-instruct-0905', 'system_fingerprint': 'fp_05df423bab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f12bc245-ff56-41b3-9600-ef58b6dfbbec-0', usage_metadata={'input_tokens': 481, 'output_tokens': 26, 'total_tokens': 507})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "\n",
    "chain.invoke({\"context\":docs,\"question\":\"apa itu rag?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e21e4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60426396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not contain any information about “rag.”'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"rag?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-service",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
