{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1807f0",
   "metadata": {},
   "source": [
    "# Penjelasan lengkap — Multi-Query (konseptual, kenapa, bagaimana) + tabel perbandingan\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Inti konsep — Apa itu Multi-Query?\n",
    "\n",
    "**Multi-Query** = membuat beberapa *query* (variasi/pecahan) dari satu pertanyaan pengguna, lalu melakukan retrieval (pencarian) secara paralel untuk tiap query. Hasil semua retrieval digabungkan, diolah, dan diserahkan ke LLM untuk disintesis menjadi jawaban akhir.\n",
    "\n",
    "Tujuan: **meningkatkan cakupan (recall) dan kualitas konteks** yang diberikan ke LLM sehingga jawaban lebih lengkap dan akurat.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Kenapa (alasan/masalah yang diselesaikan)\n",
    "\n",
    "* **Query tunggal seringkali ambigu / terlalu umum.** Satu phrasing bisa melewatkan aspek penting.\n",
    "* **Satu perspektif → blind spot.** Pertanyaan bisa memiliki beberapa dimensi (definisi, proses, contoh, implikasi) — satu query mungkin hanya menangkap beberapa dimensi.\n",
    "* **Melipatgandakan sinyal relevan.** Berbagai phrasing menarik dokumen yang berbeda → sinergi informasi.\n",
    "* **Robustness terhadap phrasing pengguna.** Paraphrase atau ekspansi membuat retrieval tidak tergantung satu formulasi kalimat.\n",
    "* **Lebih baik untuk tugas kompleks.** Untuk QA multi-step, rangkuman, analisis, dsb.\n",
    "\n",
    "Trade-off utama: **biaya & latensi naik** (lebih banyak query → lebih banyak operasi retrieval + post-processing).\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Varian Multi-Query (tipe query yang biasa dipakai)\n",
    "\n",
    "1. **Paraphrase queries** — buat variasi kalimat yang sama makna.\n",
    "   Contoh: `\"Bagaimana sistem imun merespon virus?\"` → `\"Apa tahapan respon imun terhadap infeksi virus?\"`\n",
    "\n",
    "2. **Focused sub-queries (decomposition)** — pecah pertanyaan kompleks menjadi sub-soal.\n",
    "   Contoh: `Main Q = \"Bagaimana X mempengaruhi Y?\"` → Q1: \"Apa mekanisme X?\" Q2: \"Apa bukti empiris pengaruh X→Y?\" Q3: \"Apa studi kasus?\"\n",
    "\n",
    "3. **Keyword expansion / query expansion** — tambahkan istilah kunci, sinonim, akronim.\n",
    "   Contoh: `virus → \"viral infection\", \"pathogen\", \"antiviral\"`\n",
    "\n",
    "4. **Role or intent queries** — minta perspektif berbeda: “explain like I’m 5”, “peer-reviewer perspective”, “practitioner checklist”.\n",
    "\n",
    "5. **Clarifying sub-queries** — model menghasilkan pertanyaan lanjutan yang menjelaskan konteks (mirip active clarification, kadang butuh pengguna).\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Proses end-to-end (langkah detail)\n",
    "\n",
    "Berikut alur teknisnya, langkah per langkah:\n",
    "\n",
    "1. **Input**: Pertanyaan pengguna (Q₀).\n",
    "\n",
    "2. **Query Understanding & Generation**\n",
    "\n",
    "   * Tokenize / parse Q₀.\n",
    "   * Pilih strategi (paraphrase, decomposition, expansion, dll).\n",
    "   * Gunakan LLM atau template engine untuk menghasilkan Q₁..Qₙ.\n",
    "\n",
    "3. **Retrieval (parallel)**\n",
    "\n",
    "   * Untuk setiap Qi, jalankan pencarian di vector store (embedding similarity) atau hybrid (BM25 + embeddings).\n",
    "   * Parameter: top_k per query (mis. k = 5–10).\n",
    "   * Jalankan paralel untuk mengurangi latensi wall-clock.\n",
    "\n",
    "4. **Aggregation & Deduplication**\n",
    "\n",
    "   * Gabungkan semua dokumen.\n",
    "   * Hapus duplikasi (by text hash atau cosine sim > threshold).\n",
    "   * Tambahkan metadata source (id, doc score, query asal).\n",
    "\n",
    "5. **Reranking / Filtering (opsional tapi disarankan)**\n",
    "\n",
    "   * Cross-encoder untuk re-score gabungan (lebih akurat, costlier).\n",
    "   * MMR (Maximal Marginal Relevance) untuk mengurangi redundansi dan memaksimalkan novelty.\n",
    "   * Terapkan metadata filters (time, author, domain).\n",
    "\n",
    "6. **Context Construction (prompt building)**\n",
    "\n",
    "   * Pilih top N dokumen hasil rerank (batas konteks LLM).\n",
    "   * Buat prompt: instruksi synthesize, sumber dikutip, batas panjang, target format (summary, steps, code, dsb).\n",
    "\n",
    "7. **LLM Synthesis**\n",
    "\n",
    "   * LLM menerima konteks terpilih & prompt → menghasilkan jawaban.\n",
    "   * Strategi: chain-of-thought internal (jika diperlukan), atau structured outputs (bullet points + source citations).\n",
    "\n",
    "8. **Post-processing**\n",
    "\n",
    "   * Attach provenance (sumber + skor).\n",
    "   * Confidence scoring, fallback if low confidence (minta klarifikasi atau beri opsi “I’m not sure”).\n",
    "   * Cache query results / final answer.\n",
    "\n",
    "9. **Return to User**\n",
    "\n",
    "   * Output answer + sumber referensi (link/ID + relevansi).\n",
    "\n",
    "Diagram singkat (text):\n",
    "\n",
    "```\n",
    "User Q → Query Generator → {Q1, Q2, Q3}\n",
    "   ↘         ↘         ↘\n",
    " VectorStore VectorStore VectorStore (parallel)\n",
    "   ↓           ↓           ↓\n",
    " Docs1       Docs2       Docs3 → Aggregate → Dedup → Rerank → Select topN → LLM → Answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Teknik penting & pilihan desain (detail)\n",
    "\n",
    "* **Vector store & embeddings**: pilih model embedding yang kuat untuk domain (sentence-transformers, OpenAI embeddings).\n",
    "* **Hybrid retrieval**: gabungkan sparse (BM25) + dense (embeddings) untuk manfaat keduanya.\n",
    "* **Reranker**: cross-encoder (BERT/DeBERTa) untuk akurasi re-scoring.\n",
    "* **MMR**: saat butuh info yang beragam, MMR membantu memilih dokumen yang saling melengkapi.\n",
    "* **Chunking**: dokumen panjang perlu dipotong (chunk) tapi simpan link ke source.\n",
    "* **Paraphrase generator**: bisa pakai LLM untuk buat 3–7 paraphrase.\n",
    "* **Dedup threshold**: cosine > 0.9–0.95 dianggap duplikat.\n",
    "* **Parallelism & batching**: penting untuk performa — jalankan retrieval untuk setiap query secara async.\n",
    "* **Cache**: simpan hasil retrieval untuk Q yang serupa/berulang.\n",
    "* **Cost control**: batasi jumlah query & reranker calls; gunakan cheaper retrievers + rerank hanya top-N.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Contoh pseudo-kode (sederhana)\n",
    "\n",
    "```python\n",
    "# pseudo-code\n",
    "queries = generate_queries(user_question, strategy=\"paraphrase+decompose\", n=5)\n",
    "\n",
    "# parallel retrieval\n",
    "all_docs = []\n",
    "for q in parallel(queries):\n",
    "    docs = vectorstore.search(q, top_k=8)\n",
    "    for d in docs:\n",
    "        d.origin_query = q\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "# dedup + merge\n",
    "merged = deduplicate_and_merge(all_docs, sim_threshold=0.92)\n",
    "\n",
    "# rerank (cross-encoder) on merged\n",
    "reranked = cross_encoder_rerank(merged, user_question)\n",
    "selected = select_top(reranked, n=6)\n",
    "\n",
    "# optionally apply MMR to reduce redundancy\n",
    "final_context = mmr_select(selected, user_question, k=4)\n",
    "\n",
    "# build prompt + call LLM\n",
    "prompt = build_prompt(user_question, final_context)\n",
    "answer = llm.generate(prompt)\n",
    "\n",
    "return answer, provenance=extract_provenance(final_context)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Evaluasi & metrik\n",
    "\n",
    "Ukur dengan:\n",
    "\n",
    "* **Precision@k** (proporsi dokumen relevan di top k),\n",
    "* **Recall@k** (berapa banyak info relevan ditemukan),\n",
    "* **MRR** (Mean Reciprocal Rank),\n",
    "* **F1 / ROUGE** untuk jawaban vs gold answer,\n",
    "* **Human eval** (fluency, accuracy, completeness),\n",
    "* **Latency & cost** (ms & $ per query).\n",
    "\n",
    "Untuk eksperimen, lakukan ablation: bandingkan single-query baseline vs multi-query variants.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Rekomendasi praktis singkat\n",
    "\n",
    "* Mulai dengan **3–5 queries** (paraphrase + 1 decomposed subquery).\n",
    "* Pilih **top_k = 5–8** per query, lalu dedup.\n",
    "* Gunakan reranker hanya pada gabungan (mis. rerank top 50) untuk menyeimbangkan cost.\n",
    "* Terapkan **MMR** jika jawabannya sering redundant.\n",
    "* Cache hasil retrieval untuk query yang mirip.\n",
    "* Selalu sertakan **sumber** di jawaban jika memungkinkan.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Tabel Perbandingan — Single Query vs Multi-Query Variants\n",
    "\n",
    "| Strategi                                    | Coverage (Recall) |           Precision (Noise) |                   Redundansi |       Latency |         Cost | Kompleksitas Implementasi | Cocok untuk                                |\n",
    "| ------------------------------------------- | ----------------: | --------------------------: | ---------------------------: | ------------: | -----------: | ------------------------: | ------------------------------------------ |\n",
    "| **Single Query (baseline)**                 |     Rendah–sedang |               Sedang–tinggi |                       Rendah |        Rendah |       Rendah |                    Rendah | FAQ, simple factual QA                     |\n",
    "| **Multi-Query (paraphrase)**                |     Sedang–tinggi |                      Sedang |                       Sedang |        Sedang |       Sedang |                    Sedang | Ketika phrasing pengguna bervariasi        |\n",
    "| **Multi-Query (expansion / synonyms)**      |            Tinggi | Rendah–sedang (lebih noise) |                       Tinggi |        Sedang |       Sedang |                    Sedang | Topik luas dengan banyak terminologi       |\n",
    "| **Multi-Query (decomposition/sub-queries)** |     Sangat tinggi |     Tinggi (setelah rerank) | Rendah (bila digabung smart) |  Lebih tinggi | Lebih tinggi |                    Tinggi | Pertanyaan kompleks / multi-step reasoning |\n",
    "| **Multi-Query (role/intents)**              |     Sedang–tinggi |                      Sedang |                       Sedang | Sedang–tinggi |       Sedang |                    Sedang | Butuh perspektif (practitioner vs layman)  |\n",
    "| **Multi-Query + Rerank + MMR**              |            Tinggi |                      Tinggi |                       Rendah |        Tinggi |       Tinggi |                    Tinggi | Sistem production QA / penelitian ilmiah   |\n",
    "\n",
    "**Catatan**:\n",
    "\n",
    "* *Coverage* = kemungkinan menemukan semua informasi relevan.\n",
    "* *Precision* = seberapa banyak hasil relevan (noise = non-relevan).\n",
    "* *Redundansi* = seberapa banyak hasil overlap / ulangan.\n",
    "* *Latency & Cost* naik sejalan jumlah query dan reranker usage.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Kesimpulan ringkas\n",
    "\n",
    "* **Gunakan multi-query saat kebutuhan informasi kompleks atau phrase variance tinggi.**\n",
    "* **Kontrol biaya dan kualitas** dengan reranker, MMR, dedup, dan batching.\n",
    "* **Eksperimen & ukur**: lakukan A/B antara single vs multi query di domain kamu untuk menemukan trade-off optimal.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7da0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "    \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "]\n",
    "\n",
    "loader=WebBaseLoader(\n",
    "    web_path=urls,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc48d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ce04c",
   "metadata": {},
   "source": [
    "Pertanyaan yang **bagus banget 🔥**, karena untuk *text cleaning via LLM pipeline*, model yang kamu pilih akan sangat berpengaruh pada:\n",
    "\n",
    "* **Kualitas hasil cleaning** (apakah dia benar-benar membersihkan noise, typo, dan HTML dengan baik)\n",
    "* **Biaya & kecepatan inferensi**\n",
    "* **Kemampuan multibahasa / domain tertentu**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Tujuan: *Text Cleaning via LLM Pipeline*\n",
    "\n",
    "Tujuan utama pipeline ini:\n",
    "\n",
    "> Membersihkan teks dari noise (HTML, whitespace, kalimat rusak, duplikasi, dll) sambil tetap mempertahankan makna aslinya.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Best Practice — Model Pilihan Berdasarkan Kebutuhan\n",
    "\n",
    "| Tujuan / Fokus                                           | Model yang Direkomendasikan                                 | Kelebihan                                                         | Catatan                                           |\n",
    "| -------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Cepat dan ringan (CPU-friendly)**                      | `google/flan-t5-base` / `flan-t5-large`                     | Bagus untuk normalisasi teks ringan                               | Sudah kamu pakai — cocok untuk preprocessing umum |\n",
    "| **Multilingual / Bahasa campuran (Inggris + Indonesia)** | `facebook/mbart-large-50-many-to-many-mmt`                  | Bisa memahami banyak bahasa, termasuk Indonesia                   | Butuh GPU untuk performa optimal                  |\n",
    "| **Kualitas tinggi, gaya natural dan robust**             | `mistralai/Mixtral-8x7B-Instruct-v0.1` (via API atau lokal) | Cleaning-nya lebih kontekstual (bisa perbaiki grammar, coherence) | Terlalu besar untuk lokal CPU                     |\n",
    "| **Berfokus pada kualitas teks ilmiah / artikel panjang** | `google/t5-large-ssm-nq`                                    | Training pada natural questions → hasil cleaning lebih koheren    | Bagus untuk teks akademik                         |\n",
    "| **Berbasis open-weight dan hemat resource**              | `csebuetnlp/mT5_m2m_cleaning`                               | Dibuat khusus untuk text normalization multibahasa                | Model kecil dan cepat (mirip flan-t5)             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Best Practice Pipeline (Reusable Function)\n",
    "\n",
    "Berikut contoh *best practice* function `text_cleaner()` yang siap dipakai:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Pilih model terbaik sesuai kebutuhan\n",
    "llm_cleaner = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",  # bisa diganti sesuai rekomendasi tabel\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def text_cleaner(docs, model=llm_cleaner, max_input_len=4000):\n",
    "    \"\"\"\n",
    "    Membersihkan teks dari daftar Document (LangChain) menggunakan LLM.\n",
    "    Output: DataFrame dengan kolom sumber dan hasil cleaning.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"🧹 Cleaning text with LLM\"):\n",
    "        raw_text = doc.page_content.strip()\n",
    "        prompt = (\n",
    "            \"Clean and normalize the following text by removing HTML tags, noise, \"\n",
    "            \"and redundant whitespace, but keep the original meaning:\\n\\n\"\n",
    "            + raw_text[:max_input_len]\n",
    "        )\n",
    "        try:\n",
    "            output = model(prompt, max_length=1024, truncation=True)[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            output = f\"[ERROR cleaning text: {e}]\"\n",
    "\n",
    "        results.append({\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"original_length\": len(raw_text),\n",
    "            \"cleaned_length\": len(output),\n",
    "            \"cleaned_text\": output\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Tips Pemakaian\n",
    "\n",
    "* Kalau kamu mau **multilingual cleaning**, gunakan `facebook/mbart-large-50-many-to-many-mmt` atau `mT5`.\n",
    "* Kalau teks sangat panjang (ribuan kata), bisa pakai **chunking sebelum cleaning**.\n",
    "* Untuk efisiensi, kamu bisa *cache* hasil cleaning ke `.parquet` supaya tidak perlu dijalankan ulang.\n",
    "* Hindari model yang terlalu “creative” (seperti GPT-style) untuk cleaning — karena bisa mengubah makna asli teks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fba72cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline\n",
    "\n",
    "llm_cleaner = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",  \n",
    ")\n",
    "\n",
    "def rule_based_clean(text: str) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:'\\\"()%\\-]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def text_cleaner(docs, model=None, use_llm=False):\n",
    "    cleaned_docs = []\n",
    "    for doc in tqdm(docs, desc=\"🧹 Cleaning Docs (Fast Mode)\"):\n",
    "        raw = doc.page_content.strip()\n",
    "        cleaned = rule_based_clean(raw)\n",
    "        if use_llm and model is not None:\n",
    "            prompt = (\n",
    "                \"Refine and improve readability of this cleaned text \"\n",
    "                \"without changing its meaning:\\n\\n\" + cleaned\n",
    "            )\n",
    "            try:\n",
    "                cleaned = model(prompt, max_length=512, truncation=True)[0][\"generated_text\"]\n",
    "            except Exception as e:\n",
    "                cleaned = f\"[ERROR refining text: {e}]\"\n",
    "        cleaned_docs.append(Document(page_content=cleaned, metadata=doc.metadata))\n",
    "    return cleaned_docs\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "871dafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning Docs (Fast Mode): 100%|██████████| 222/222 [00:00<00:00, 6228.58it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(docs)\n",
    "cleaned_docs = text_cleaner(docs, model=llm_cleaner, use_llm=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-service",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
